{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMSCE DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simcse import SimCSE\n",
    "model = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0074e-02, -3.1106e-02, -4.1434e-02,  5.4180e-02,  3.8284e-03,\n",
       "        -2.2707e-02,  8.6960e-03,  1.3826e-02,  4.2733e-02,  3.4025e-02,\n",
       "         1.2988e-02, -4.0907e-02,  1.0693e-02, -4.7533e-02,  4.6440e-02,\n",
       "        -9.8162e-03, -7.6669e-03, -1.9164e-02,  1.3691e-03, -6.2468e-02,\n",
       "         4.9623e-02,  3.2637e-02,  3.8227e-02, -1.8658e-02, -1.5207e-02,\n",
       "         3.2509e-02,  1.3643e-02,  1.5067e-02,  4.0355e-02, -3.6133e-02,\n",
       "         2.2097e-03, -1.0038e-02,  4.8693e-02,  1.0413e-02,  7.0862e-02,\n",
       "         1.2006e-02, -1.3823e-02, -2.8470e-02, -4.4552e-02,  7.3731e-02,\n",
       "        -5.0263e-02, -8.7763e-03,  6.5772e-02,  3.7907e-03, -1.3432e-02,\n",
       "         5.8235e-02, -4.8969e-02,  5.3680e-02,  1.3781e-02, -3.6177e-02,\n",
       "         4.4991e-02, -3.5547e-02, -7.7161e-02,  2.4293e-02, -2.8086e-03,\n",
       "        -4.4119e-02,  1.9969e-03, -1.5316e-02,  9.0253e-06,  1.7756e-02,\n",
       "         6.2693e-03, -6.9084e-03, -6.5714e-03,  9.3938e-03,  2.3357e-02,\n",
       "        -2.3919e-02,  1.0355e-03, -6.3004e-02,  5.1440e-03,  8.9753e-04,\n",
       "        -6.9116e-02,  4.3042e-02,  9.4407e-03,  1.5697e-02, -2.6217e-02,\n",
       "         3.4887e-02,  1.5513e-02,  7.5747e-03, -2.3958e-02, -3.9667e-02,\n",
       "         1.1027e-02, -3.0505e-03, -4.4712e-02, -2.4989e-02,  1.8112e-03,\n",
       "        -1.1073e-02,  7.0167e-02,  4.7032e-02,  5.3936e-02,  6.6120e-02,\n",
       "        -4.6919e-02, -7.6359e-02, -3.8901e-02, -1.0274e-02,  3.1475e-03,\n",
       "        -3.0803e-03,  6.6377e-03, -6.5148e-02,  3.2420e-02,  3.6182e-02,\n",
       "        -5.6617e-03, -1.8498e-02,  5.2200e-03, -4.2575e-02, -1.7748e-02,\n",
       "        -5.5327e-02,  7.0112e-03,  1.1854e-02, -2.3251e-02,  2.1849e-02,\n",
       "        -3.7297e-02, -5.3283e-03, -1.0232e-02, -6.3227e-02,  4.6631e-02,\n",
       "         4.2051e-02,  6.1741e-02,  6.9677e-02,  8.0846e-02, -5.1363e-02,\n",
       "         2.0627e-02, -2.9667e-03,  4.4007e-02,  6.3960e-02,  2.5733e-02,\n",
       "         2.5712e-02, -4.1203e-02,  2.2531e-03, -1.3955e-02,  3.0849e-02,\n",
       "        -1.0919e-02, -2.9341e-02, -1.2796e-03,  6.5722e-02,  4.0777e-02,\n",
       "        -8.7309e-03, -3.5073e-02, -1.7974e-02,  9.4529e-03, -2.7654e-02,\n",
       "         2.3432e-02,  3.8992e-02, -2.0162e-02, -7.7306e-04,  2.3646e-02,\n",
       "         1.7321e-02,  4.9841e-02,  4.4320e-02, -3.2837e-02, -4.3794e-02,\n",
       "         5.7998e-02,  7.1527e-02,  2.8954e-02, -4.1220e-02,  6.4208e-02,\n",
       "         7.2556e-03,  2.5633e-02, -6.0635e-03,  2.2818e-02,  6.0529e-02,\n",
       "         3.4603e-02, -3.9261e-03,  7.8231e-02,  5.5909e-03,  2.1200e-02,\n",
       "        -3.3435e-02, -3.5152e-02,  9.8388e-03,  7.9036e-03, -4.8957e-02,\n",
       "        -2.5411e-02,  9.0172e-03,  1.2803e-03,  3.2260e-02, -4.0865e-02,\n",
       "        -3.4796e-03,  4.7425e-03,  5.3375e-03, -3.1547e-02, -2.6191e-02,\n",
       "        -1.4926e-02,  4.1103e-02,  7.4629e-02,  6.6630e-02, -9.8885e-03,\n",
       "         9.1941e-04,  5.4640e-02, -1.7128e-02,  3.0621e-02,  7.2556e-03,\n",
       "        -1.4960e-02, -3.1613e-02, -6.9330e-02,  2.8291e-02,  1.8437e-02,\n",
       "        -3.0217e-02,  4.2538e-02,  1.1882e-02, -7.7238e-03, -3.5691e-02,\n",
       "         2.9412e-02, -1.3017e-02, -2.2457e-02,  1.2477e-02, -4.5585e-02,\n",
       "        -6.1012e-03, -3.8083e-02,  3.5658e-02,  4.3747e-02,  2.5615e-02,\n",
       "        -3.8793e-02,  1.6540e-02, -5.6234e-02, -9.0475e-03,  2.5749e-02,\n",
       "         1.1570e-02, -2.3165e-04, -3.7011e-03,  1.0010e-02, -2.9887e-02,\n",
       "         4.9481e-02, -4.3677e-03,  3.6762e-02, -3.6421e-02, -8.3289e-03,\n",
       "         3.4516e-02, -3.8180e-02, -1.9378e-02, -4.4953e-03, -1.1415e-02,\n",
       "         3.0241e-03, -1.0367e-03, -9.9858e-02, -2.2846e-02, -4.4542e-02,\n",
       "         1.6636e-02,  3.3434e-02, -2.0096e-02, -1.5456e-02,  4.6556e-02,\n",
       "         1.4325e-02,  4.0673e-02,  1.1092e-03, -5.2610e-02, -4.1712e-02,\n",
       "        -3.2603e-02, -2.5971e-02, -3.8275e-02,  5.8367e-02, -2.8422e-02,\n",
       "        -2.6465e-02, -5.7573e-03, -4.2708e-02,  7.5931e-03,  8.5972e-03,\n",
       "         2.3513e-02, -3.0450e-02, -2.1947e-02,  2.7808e-02,  6.6591e-03,\n",
       "         5.9580e-03,  1.6389e-02, -1.4820e-02,  2.2230e-02,  1.6567e-02,\n",
       "        -3.7796e-02,  1.6645e-02, -2.3798e-03,  3.5227e-02, -6.8206e-04,\n",
       "        -8.4673e-03,  2.1324e-03,  5.6482e-02,  1.6699e-02,  8.1987e-02,\n",
       "         2.3633e-02, -3.9932e-02, -3.4124e-02,  1.3908e-02,  4.3313e-02,\n",
       "        -2.2625e-03, -1.3725e-02,  1.2612e-02,  4.9817e-02,  4.2958e-03,\n",
       "        -6.5894e-02,  3.4655e-02, -2.1331e-02,  3.7665e-02,  4.6671e-02,\n",
       "        -3.9345e-02,  1.1122e-02,  5.3430e-03,  7.5515e-03,  2.5941e-02,\n",
       "        -1.2835e-02,  4.3230e-03,  9.1722e-02, -6.2612e-02, -1.0840e-02,\n",
       "         4.5175e-02, -6.5446e-02, -1.1531e-02, -4.2083e-02,  5.1615e-02,\n",
       "         7.2402e-03, -2.3102e-02,  3.7679e-03,  3.7187e-03, -2.5113e-02,\n",
       "         1.5918e-03, -6.4016e-02, -7.0173e-03, -5.8985e-03, -7.1273e-03,\n",
       "        -4.4810e-02,  2.3264e-02, -7.6106e-03,  4.5087e-04,  1.8944e-02,\n",
       "         3.3031e-02,  3.4151e-02,  2.9655e-02, -4.7328e-02, -1.9726e-02,\n",
       "        -2.6558e-02, -1.3357e-02,  5.9608e-02, -3.8512e-02, -3.5962e-03,\n",
       "        -6.2978e-02, -1.6847e-02, -4.2232e-02, -5.4712e-02,  2.2700e-03,\n",
       "         4.2157e-03, -3.2392e-02, -5.8744e-02,  5.7439e-02,  4.3461e-02,\n",
       "        -5.8895e-02, -2.6540e-02,  3.4329e-02,  6.4528e-04,  1.6057e-02,\n",
       "        -2.2332e-03,  2.5141e-02,  1.7026e-02, -2.1115e-02,  3.2400e-02,\n",
       "        -3.7537e-02,  2.8602e-02,  1.0648e-02, -2.5340e-02, -7.8534e-02,\n",
       "        -1.6538e-02,  6.7627e-02, -3.0814e-02,  2.8192e-02, -1.3674e-03,\n",
       "        -1.3973e-02, -1.5550e-03,  3.8746e-02,  2.8606e-02,  3.7922e-03,\n",
       "        -2.4404e-02, -2.4790e-02, -1.8742e-02, -4.8143e-02,  3.8799e-02,\n",
       "        -7.8519e-03,  5.5783e-02,  1.4928e-02,  5.1274e-03,  6.7918e-02,\n",
       "        -3.1157e-02,  4.5346e-02, -3.4638e-02, -3.6830e-02, -4.2242e-04,\n",
       "        -2.1523e-02,  1.6760e-02,  1.4672e-02, -2.7012e-02,  3.2486e-02,\n",
       "         4.2023e-02,  5.4787e-03,  5.1256e-02, -3.5609e-02,  1.0785e-02,\n",
       "         8.5253e-03, -4.0274e-02, -4.4713e-02,  9.6566e-02,  2.6181e-02,\n",
       "         7.2021e-03,  3.1581e-02, -2.2149e-02,  2.1398e-02,  6.8181e-04,\n",
       "         4.0173e-02, -3.1470e-04,  3.8572e-02,  7.3054e-02, -2.2874e-02,\n",
       "        -4.7880e-02, -5.7582e-03, -8.3467e-02, -2.0848e-02,  8.2321e-03,\n",
       "        -1.7754e-02,  8.7106e-02, -5.5840e-02, -2.2004e-02,  5.5987e-02,\n",
       "         2.6393e-02, -8.4556e-03, -1.5182e-02,  4.1928e-02, -3.2306e-02,\n",
       "         4.4774e-03, -8.5148e-02, -4.9530e-02,  1.5720e-02,  2.1683e-02,\n",
       "         2.2167e-02, -2.6121e-02,  2.8580e-02, -1.7152e-02,  8.2604e-03,\n",
       "         4.5344e-02,  1.3516e-02, -4.7560e-04, -6.5669e-04, -1.3479e-02,\n",
       "         3.8609e-02, -2.7362e-02,  4.5733e-02,  3.1221e-03,  5.3320e-02,\n",
       "         2.7140e-02, -2.2064e-02,  6.5730e-02, -4.7258e-02, -4.5683e-02,\n",
       "        -1.4241e-02,  6.1835e-03,  6.3371e-02,  7.1375e-02, -6.2181e-03,\n",
       "        -8.8245e-02,  3.3511e-02,  3.5349e-02,  1.6410e-02,  2.0777e-02,\n",
       "        -6.5824e-03, -2.2090e-02, -1.8547e-02, -1.2259e-02,  2.0029e-02,\n",
       "        -5.7794e-02,  2.2885e-02, -5.8174e-03,  1.3950e-02, -6.4180e-03,\n",
       "         1.9528e-02, -1.4529e-02,  4.2532e-02, -3.8609e-03,  2.0449e-02,\n",
       "        -2.2148e-03, -2.9622e-02, -2.9175e-02, -2.0066e-02, -3.3776e-02,\n",
       "        -1.7101e-02, -2.5771e-02, -3.3054e-02, -4.0554e-02, -6.3049e-02,\n",
       "        -8.0225e-03,  2.5581e-02, -4.2280e-02,  3.4717e-02,  3.8700e-02,\n",
       "        -2.4172e-02, -2.7234e-02, -3.2092e-02,  7.2591e-03,  1.8026e-02,\n",
       "        -5.6504e-03, -1.0927e-02,  5.0280e-02,  2.2019e-02,  4.9516e-02,\n",
       "        -1.6982e-02, -4.5367e-02, -6.3875e-03,  1.8660e-02,  4.6249e-02,\n",
       "         3.8761e-02, -6.7765e-02, -6.6267e-02, -5.7254e-03,  9.2240e-03,\n",
       "        -7.4391e-03,  2.5000e-03,  3.6483e-02, -1.1802e-02, -1.8366e-02,\n",
       "        -3.9597e-02, -3.4861e-02,  5.1821e-03,  2.6531e-02,  6.0425e-03,\n",
       "         5.9161e-02,  4.3072e-04,  3.7713e-02, -6.8790e-03,  5.6744e-02,\n",
       "         4.3878e-02, -2.7607e-02,  6.2519e-02, -4.3692e-02,  7.1127e-02,\n",
       "        -2.7385e-02, -2.6352e-02,  1.8696e-02,  1.3988e-02, -1.0486e-02,\n",
       "        -1.7035e-03,  8.1991e-03,  5.0731e-02, -3.4386e-02,  2.4027e-02,\n",
       "         1.6984e-02,  1.3671e-02, -2.9248e-02,  4.6618e-02,  3.5922e-02,\n",
       "        -3.4155e-02, -1.8969e-03,  7.8749e-03,  4.4389e-02,  9.1033e-03,\n",
       "        -1.0738e-02,  8.9721e-02, -6.3094e-02,  3.3362e-02, -1.6868e-02,\n",
       "        -6.7564e-03,  2.8502e-02, -2.0962e-02,  4.2398e-02,  4.1904e-02,\n",
       "        -8.0146e-02,  1.5203e-02, -2.0775e-02,  1.3787e-02,  1.3580e-02,\n",
       "        -8.6384e-03, -4.1313e-03, -3.4160e-02,  1.7764e-02,  1.1355e-02,\n",
       "        -5.1731e-02,  2.7437e-02,  3.6666e-03, -2.4539e-02, -5.6486e-02,\n",
       "         1.2243e-02, -6.5941e-02, -8.5724e-03,  1.1288e-02,  3.7000e-02,\n",
       "        -3.3109e-02,  3.1501e-02,  1.7411e-02, -6.2899e-03,  3.2786e-02,\n",
       "         1.8945e-02, -1.5425e-02,  3.1750e-02, -7.0106e-03, -4.1427e-02,\n",
       "         6.9270e-03, -8.0356e-03, -3.8285e-02,  3.2003e-03, -7.0774e-02,\n",
       "        -5.1527e-02,  4.3822e-02, -9.3966e-03,  3.1379e-02, -5.0235e-02,\n",
       "        -1.9341e-02,  2.9290e-02,  5.4579e-02,  4.1907e-02,  6.0716e-02,\n",
       "        -5.2393e-02,  1.7997e-02,  5.7971e-02,  4.9119e-02, -1.0420e-02,\n",
       "         1.2131e-02,  1.0286e-02, -2.7462e-02,  6.2258e-02, -4.5314e-02,\n",
       "         1.1336e-04, -8.2438e-02,  8.3345e-03, -1.2970e-02,  1.6142e-02,\n",
       "        -3.5933e-02,  2.6776e-02, -3.2721e-02, -4.4158e-02,  9.6562e-02,\n",
       "        -6.7862e-02, -4.1502e-02, -3.3158e-02, -6.4018e-02,  1.4779e-02,\n",
       "         9.7248e-02,  4.4524e-02, -2.1229e-02, -7.6165e-03,  5.0979e-02,\n",
       "        -6.8168e-03,  1.4454e-03, -8.6853e-03,  4.6841e-02,  2.3566e-02,\n",
       "         9.2404e-02,  4.3574e-02, -4.8392e-03, -8.7041e-04,  3.0324e-02,\n",
       "         1.4596e-02, -2.3894e-03, -3.2801e-02, -3.2661e-02,  4.1860e-02,\n",
       "        -5.7775e-02,  1.9854e-02,  1.3061e-02, -2.2236e-02, -1.4992e-02,\n",
       "         1.3559e-02,  2.6891e-02, -3.3508e-02, -3.6359e-02,  7.2577e-02,\n",
       "        -2.2269e-02, -1.7890e-03,  1.8504e-02, -6.6001e-03,  5.9002e-02,\n",
       "         5.6630e-02, -1.1625e-02, -7.3545e-02, -5.6742e-02,  1.7350e-02,\n",
       "        -2.2214e-02, -2.9217e-02,  2.8636e-03,  1.5042e-02,  5.4604e-03,\n",
       "         4.6059e-02,  6.8805e-03, -1.3745e-02,  5.5079e-02, -2.8653e-03,\n",
       "        -2.2162e-02,  7.7766e-04,  7.9728e-03,  3.4105e-02, -4.5827e-02,\n",
       "        -1.5820e-02,  4.6236e-02,  7.2639e-03,  3.3021e-03, -3.8379e-02,\n",
       "        -2.3227e-02, -6.4805e-03,  7.8072e-03,  3.8012e-02,  5.4170e-03,\n",
       "        -2.0100e-03,  1.2610e-02, -6.1804e-02, -7.4643e-03,  6.0075e-04,\n",
       "         3.5577e-02,  1.2644e-02,  7.1019e-02, -4.2716e-03,  5.4496e-02,\n",
       "        -2.3156e-02, -5.6068e-03, -7.3772e-02,  1.5935e-02,  2.4560e-02,\n",
       "         2.3267e-02,  4.5755e-02,  1.4478e-02,  6.0918e-02,  1.9826e-02,\n",
       "         2.6593e-02,  3.9601e-02,  2.8437e-02,  4.2485e-03,  2.1074e-02,\n",
       "         7.4675e-02, -2.8638e-02, -1.0042e-01, -6.4349e-05,  4.6027e-03,\n",
       "         6.6624e-02,  6.3311e-02,  3.3282e-02, -2.1033e-03,  5.3674e-02,\n",
       "         2.4192e-03,  4.8876e-02,  6.4400e-04, -6.1667e-02,  1.5246e-02,\n",
       "         4.5699e-03, -6.6097e-02, -1.3338e-01,  3.9986e-02,  9.0365e-03,\n",
       "         2.9924e-02, -3.1271e-03, -3.8765e-02, -1.5407e-02, -1.3600e-02,\n",
       "         9.4880e-02,  1.2484e-02, -4.1350e-02,  3.0508e-02,  4.7262e-03,\n",
       "        -5.3219e-02,  2.3765e-03, -1.0580e-02, -1.7960e-02,  1.3635e-03,\n",
       "        -4.0705e-02,  4.2764e-04,  3.8597e-02, -3.1975e-02,  5.6366e-02,\n",
       "        -9.1494e-03, -3.7096e-02, -6.2482e-02,  2.1399e-02, -2.6459e-03,\n",
       "         2.1338e-02,  4.5500e-02,  5.2467e-02, -1.5080e-02, -6.9236e-02,\n",
       "         2.7500e-02,  1.7867e-02,  5.0263e-02])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.encode(\"A woman is reading.\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01262088, 0.34469506],\n",
       "       [0.89384246, 0.04842845]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_a = ['A woman is reading.', 'A man is playing a guitar.']\n",
    "sentences_b = ['He plays guitar.', 'A woman is making a photo.']\n",
    "similarities = model.similarity(sentences_a, sentences_b)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/30/2024 19:38:45 - INFO - simcse.tool -   Encoding embeddings for sentences...\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "11/30/2024 19:38:45 - INFO - simcse.tool -   Building index...\n",
      "11/30/2024 19:38:45 - INFO - simcse.tool -   Use CPU-version faiss\n",
      "11/30/2024 19:38:45 - INFO - simcse.tool -   Finished\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['A woman is reading.', 'A man is playing a guitar.']\n",
    "model.build_index(sentences)\n",
    "results = model.search(\"he is reading book\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMSCE DEFAULT dengan menggunakan huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='princeton-nlp/sup-simcse-bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: There's a kid on a skateboard.\n",
      "Tokenized words: ['there', \"'\", 's', 'a', 'kid', 'on', 'a', 'skate', '##board', '.']\n",
      "\n",
      "Original text: A kid is skateboarding.\n",
      "Tokenized words: ['a', 'kid', 'is', 'skate', '##boarding', '.']\n",
      "\n",
      "Original text: A kid is inside the house.\n",
      "Tokenized words: ['a', 'kid', 'is', 'inside', 'the', 'house', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A kid is skateboarding.\",\n",
    "    \"A kid is inside the house.\"\n",
    "]\n",
    "tokenized_words = [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "# Print the tokenized words\n",
    "for i, tokens in enumerate(tokenized_words):\n",
    "    print(f\"Original text: {texts[i]}\")\n",
    "    print(f\"Tokenized words: {tokens}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2045,  1005,  1055,  1037,  4845,  2006,  1037, 17260,  6277,\n",
       "          1012,   102],\n",
       "        [  101,  1037,  4845,  2003, 17260, 21172,  1012,   102,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  1037,  4845,  2003,  2503,  1996,  2160,  1012,   102,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is skateboarding.\" is: 0.943\n",
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is inside the house.\" is: 0.439\n",
      "Cosine similarity between \"A kid is skateboarding.\" and \"A kid is inside the house.\" is: 0.454\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "cosine_sim_1_2 = 1 - cosine(embeddings[1], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[1], texts[2], cosine_sim_1_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMSCE LOCAL Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='result/my-sup-simcse-bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"result/my-sup-simcse-bert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/my-sup-simcse-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the tokenizer and model\n",
    "\n",
    "model = AutoModel.from_pretrained(\"result/my-sup-simcse-bert-base-uncased\")\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: There's a kid on a skateboard.\n",
      "Tokenized words: ['there', \"'\", 's', 'a', 'kid', 'on', 'a', 'skate', '##board', '.']\n",
      "\n",
      "Original text: A kid is skateboarding.\n",
      "Tokenized words: ['a', 'kid', 'is', 'skate', '##boarding', '.']\n",
      "\n",
      "Original text: A kid is inside the house.\n",
      "Tokenized words: ['a', 'kid', 'is', 'inside', 'the', 'house', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A kid is skateboarding.\",\n",
    "    \"A kid is inside the house.\"\n",
    "]\n",
    "tokenized_words = [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "# Print the tokenized words\n",
    "for i, tokens in enumerate(tokenized_words):\n",
    "    print(f\"Original text: {texts[i]}\")\n",
    "    print(f\"Tokenized words: {tokens}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2045,  1005,  1055,  1037,  4845,  2006,  1037, 17260,  6277,\n",
       "          1012,   102],\n",
       "        [  101,  1037,  4845,  2003, 17260, 21172,  1012,   102,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  1037,  4845,  2003,  2503,  1996,  2160,  1012,   102,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is skateboarding.\" is: 0.953\n",
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is inside the house.\" is: 0.660\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMSCE LOCAL exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/exp1 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is skateboarding.\" is: 0.948\n",
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is inside the house.\" is: 0.682\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Path to the directory where the model is saved\n",
    "model_path = \"result/exp1\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A kid is skateboarding.\",\n",
    "    \"A kid is inside the house.\"\n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMSCE LOCAL exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/exp2 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is skateboarding.\" is: 0.940\n",
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is inside the house.\" is: 0.603\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Path to the directory where the model is saved\n",
    "model_path = \"result/exp2\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A kid is skateboarding.\",\n",
    "    \"A kid is inside the house.\"\n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMSCE LOCAL exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/exp3 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is skateboarding.\" is: 0.957\n",
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is inside the house.\" is: 0.461\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Path to the directory where the model is saved\n",
    "model_path = \"result/exp3\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A kid is skateboarding.\",\n",
    "    \"A kid is inside the house.\"\n",
    "    \n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMSCE LOCAL Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at result/my-unsup-simcse-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is skateboarding.\" is: 0.898\n",
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is inside the house.\" is: 0.717\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Path to the directory where the model is saved\n",
    "model_path = \"result/my-unsup-simcse-bert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A kid is skateboarding.\",\n",
    "    \"A kid is inside the house.\"\n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is skateboarding.\" is: 0.804\n",
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A kid is inside the house.\" is: 0.797\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Path to the directory where the model is saved\n",
    "model_path = \"microsoft/mpnet-base\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A kid is skateboarding.\",\n",
    "    \"A kid is inside the house.\"\n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simsceenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
